{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN with Laplace Beltrami Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification: Males vs. Females"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to see if we can create a 1 dimensional convolutional neural network that can classify a subject as male or female based on the shape (LB spectrum) of their white matter tracts.\n",
    "\n",
    "The initial set up will be very similar to the MLP as we want the data to remain as a 1D vector. The main changes will be the types of layers in the actual network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "First, let's import the libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named seaborn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d1b54c62310f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#for plotting, numbers etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#for splitting the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named seaborn"
     ]
    }
   ],
   "source": [
    "#to read in the data\n",
    "import pickle\n",
    "#for plotting, numbers etc.\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#for splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "#keras functions\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.utils import np_utils, plot_model, to_categorical\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and check the data\n",
    "Now let's read in the data using pickle. The data was previously processed in python and saved down using pickle. This will be the same steps as used in the MLP notebook so I will not include as many comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvalue dictionary with entry for eact tract, 600 evs per tract\n",
    "tractev_dict_600 = pickle.load(open(\"tract_ev_dict_600.pk\",'rb'))\n",
    "# list of tracts we want to use\n",
    "tractstouse = pickle.load(open('tractstouse.pk','rb'))\n",
    "# subject list\n",
    "HCP_subj_list = pickle.load(open('HCP_subj_list.pk','rb'))\n",
    "# list of subject gender 1 = male, 2 = female\n",
    "gender_id = pickle.load(open('gender_id.pk','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalue data is already in a vector format, so we do not need to vectorize it. However, we will need to combine the vectors of all the tracts so that we have a single vector per subject. \n",
    "\n",
    "We also need to normalize the data so that each set of eigenvalues has a mean of 0 and a standard deviation of 1. We will write a function to do this using sklearn's `StandardScaler` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_ev_dict(ev_dict):\n",
    "    scaled_dict = {}\n",
    "    for tract in ev_dict.keys():\n",
    "        scaler = StandardScaler()\n",
    "        scaled_dict[tract] = scaler.fit_transform(ev_dict[tract])\n",
    "    return scaled_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3a9666b08d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# normalize all of the tracts so that each ev is centered on 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtractev_dict_600_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_ev_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtractev_dict_600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-b68d31e5235f>\u001b[0m in \u001b[0;36mscale_ev_dict\u001b[0;34m(ev_dict)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mscaled_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtract\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mev_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mscaled_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtract\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mev_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtract\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscaled_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "# normalize all of the tracts so that each ev is centered on 0.\n",
    "tractev_dict_600_scaled = scale_ev_dict(tractev_dict_600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reorganize the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the data is a dictionary of 2D matrices, we want to reorganize this into a single 2D matrix with the shape `(1013, n * 48)`, where n is the number of eigenvalues we are using. It is likely that 600 eigenvalues is way more than we need, but we do not know how many eigenvalues is optimal. We will write a function to do this reorganization so we can easily try multiple amounts of eigenvalues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the organization to be one vector per subject with all evs for all tracts\n",
    "def reorganize_spectrums(ev_dict_scaled, numev, HCP_subj_list=HCP_subj_list, tractstouse=tractstouse):\n",
    "    # create an empty numpy array of the shape we want\n",
    "    # numev is the number of eigenvalues we want per tract\n",
    "    allsubjs_alltracts_scaled = np.zeros([len(HCP_subj_list), numev*len(tractstouse)])\n",
    "    for i in range(len(tractstouse)):\n",
    "        allsubjs_alltracts_scaled[:, i*numev:i*numev+numev] = ev_dict_scaled[tractstouse[i]][:, 0:numev]\n",
    "    return allsubjs_alltracts_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numev=200\n",
    "allsubjs_alltracts_scaled = reorganize_spectrums(tractev_dict_600_scaled, numev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One hot encoding the labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To one hot encoding these labels all we need to do is simple subtract 1 from every entry. Now 0 = male and 1 = female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding for the gender ID\n",
    "genderid_ohe = np.asarray(gender_id) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check datatype**\n",
    "\n",
    "Finally, we need to make sure all inputs are of datatype `float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsubjs_alltracts_scaled = allsubjs_alltracts_scaled.astype('float32')\n",
    "genderid_ohe = genderid_ohe.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "\n",
    "The input data is now preprocessed and ready to be input into a neural network. However, we first have to split the data into training, validation, and testing sets. We do not have a ton of samples, so we will try splitting the data into 3 subsets first and then explore other cross validation options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = allsubjs_alltracts_scaled\n",
    "Y = genderid_ohe\n",
    "\n",
    "#first split the training/validation data from the testing data\n",
    "trainvalX, testX, trainvalY, testY = train_test_split(X, Y, train_size  = .8, test_size = .2, random_state=0)\n",
    "\n",
    "print len(trainvalX) \n",
    "print len(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second split the validation data from the training data\n",
    "trainX, valX, trainY, valY = train_test_split(trainvalX, trainvalY, train_size  = .75, test_size = .25, random_state=0)\n",
    "\n",
    "print len(trainX)\n",
    "print len(valX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 3 subsets of data. Training data with 607 samples, validation data with 203 samples, and testing data with 203 samples. Again, this may be too few samples for training and if so we can employ other cross validation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Logistic Regression Classifier from sklearn for comparison\n",
    "\n",
    "One of the reasons we are trying deep learning with this data is to see if we can improve upon 'basic' machine learning algorithms. Lets try this data with a simple Logistic Regression classifier so we have something to compare the deep learning accuracy to. We will train and evaluate the training and validation splits as well as the training/val and testing splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on training data\n",
    "lr = LogisticRegressionCV()\n",
    "lr.fit(trainX, trainY)\n",
    "\n",
    "#evaluate with validation data\n",
    "print(\"Accuracy = {:.2f}\".format(lr.score(valX, valY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on training and validation data\n",
    "lr = LogisticRegressionCV()\n",
    "lr.fit(trainvalX, trainvalY)\n",
    "\n",
    "#evaluate with validation data\n",
    "print(\"Accuracy = {:.2f}\".format(lr.score(testX, testY)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.4py2.7]",
   "language": "python",
   "name": "conda-env-tf1.4py2.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
